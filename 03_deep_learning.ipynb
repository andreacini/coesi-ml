{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/andreacini/coesi-ml/blob/master/03_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RRuwG7zn_YO_"
   },
   "source": [
    "# Exercise 03: Classification and Deep Neural Networks\n",
    "\n",
    "In this lab we are going to focus on some practical aspects of building deep neural networks. \n",
    "\n",
    "We will focus on three main tasks: \n",
    "\n",
    "1. Building a **dense** classifier for images of numerical digits;\n",
    "2. Building a **convolutional** classifier for images of numerical digits;\n",
    "3. **Fine-tuning** a deep network to classify images of cats and dogs;\n",
    "\n",
    "We will use TF/Keras throughout this lab, but the same things can be done with little effort in any deep learning frameworks. As usual, we encourage you to try and implement everything on your own from scratch.\n",
    "\n",
    "Let's get started..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30RKBWXz_xAE"
   },
   "source": [
    "## MNIST\n",
    "\n",
    "The **Modified National Institute of Standards and Technology database** is a large collection of handwritten digits that is widely used in machine learning as a benchmark for computer vision algorithms.   \n",
    "The dataset consists of 70000 images of handwritted digits. All images are 28 pixels by 28 pixels, in 8-bit grayscale (i.e., each pixel is represented by an integer value in the 0-255 range), and are equally divided into 10 classes.\n",
    "\n",
    "MNIST is usually considered as a multi-class classification problem, where the goal is to map each image to its corresponding class. \n",
    "\n",
    "Although nowadays MNIST is regarded as solved, machine learning practitioners like to joke that while it's true that if something works on MNIST, it may not work in the real world, it is also true that if it **doesn't** work on MNIST, it will surely not work in the real world.\n",
    "\n",
    "\n",
    "Let's look at the data...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trKY2GuMtoBF"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(20)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Load the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(131)\n",
    "plt.imshow(x_train[4], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(132)\n",
    "plt.imshow(x_train[6], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(133)\n",
    "plt.imshow(x_train[7], cmap='gray');\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOftvishuRlz"
   },
   "source": [
    "## Dense classifier\n",
    "Now let's build a neural network with the tools that we have seen so far, i.e., using only dense layers.\n",
    "\n",
    "We start by pre-processing the images and reshaping them as vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUXGWpOsuueT"
   },
   "outputs": [],
   "source": [
    "# Reshape to vectors\n",
    "x_train = x_train.reshape(-1, 28 * 28)  # shape: (60000, 784)\n",
    "x_test = x_test.reshape(-1, 28 * 28)    # shape: (10000, 784)\n",
    "\n",
    "# Normalize to 0-1 range\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T-T8mzp9u72X"
   },
   "source": [
    "We also have to pre-process our targets in order to perform multi-class classification. We will use **one-hot encoding** to represent our numerical labels (0-9) as sparse binary vectors. For instance, the one-hot encoding of label 3 will be $[0, 0, 0, 1, 0 ,0 ,0, 0, 0, 0]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zD-HkP-evib0"
   },
   "outputs": [],
   "source": [
    "# Pre-process targets\n",
    "from keras import utils\n",
    "n_classes = 10\n",
    "y_train = utils.to_categorical(y_train, n_classes)\n",
    "y_test = utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QfoJd8iKvi2l"
   },
   "source": [
    "Now we build a neural classifier using the same tools that we saw in the previous lab. Remember that we reshaped our inputs to be vectors, so we are in the same familiar setting as always.\n",
    "\n",
    "However, this time we will be dealing with multi-class classification, which means that our output layer will have 10 possible outputs instead of a single one.\n",
    "Moreover, the sigmoid activation function that we used in our previous binary classifiers will be replaced by the normalized **softmax** function, which will give us a **probability distribution** over the possible labels:\n",
    "\n",
    "$$\n",
    "\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "where $K$ is the number of classes that we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYsaysq4v_mO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "\n",
    "# Build model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='tanh', input_shape=(28*28, )))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=optimizers.SGD(lr=0.01), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wz8ikRFXwKn7"
   },
   "source": [
    "We can now train and evaluate the model using Keras' `fit` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NUDsIDiRwP_S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 2s 37us/step - loss: 0.6539 - acc: 0.8352 - val_loss: 0.3403 - val_acc: 0.9108\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 2s 31us/step - loss: 0.3723 - acc: 0.8968 - val_loss: 0.2817 - val_acc: 0.9245\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 2s 31us/step - loss: 0.3268 - acc: 0.9080 - val_loss: 0.2564 - val_acc: 0.9305\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 2s 32us/step - loss: 0.3013 - acc: 0.9151 - val_loss: 0.2390 - val_acc: 0.9345\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.2820 - acc: 0.9203 - val_loss: 0.2255 - val_acc: 0.9375\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 2s 32us/step - loss: 0.2665 - acc: 0.9244 - val_loss: 0.2155 - val_acc: 0.9408\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 2s 31us/step - loss: 0.2524 - acc: 0.9287 - val_loss: 0.2040 - val_acc: 0.9422\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 2s 31us/step - loss: 0.2398 - acc: 0.9322 - val_loss: 0.1944 - val_acc: 0.9478\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 2s 31us/step - loss: 0.2282 - acc: 0.9347 - val_loss: 0.1867 - val_acc: 0.9505\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 2s 32us/step - loss: 0.2175 - acc: 0.9381 - val_loss: 0.1805 - val_acc: 0.9523\n",
      "10000/10000 [==============================] - 0s 14us/step\n",
      "Test loss: 0.20747968491017818 - Accuracy: 0.9407\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          validation_split=0.1)\n",
    "\n",
    "# Evaluate model\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print('Test loss: {} - Accuracy: {}'.format(*scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xNoK7dVwYWB"
   },
   "source": [
    "So far, so good, right? Everything was in our domain of competence and we were able to build a pretty good classifier.\n",
    "\n",
    "However, we can easily see that our classifier is actually not as good as we think. In particular, we can show that the dense network does not have the property of **translation invariance**, meaning that if we move the content of our images around too much, the classifier quickly fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sut8Ugzr-XPm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 14us/step\n",
      "Test loss: 4.996412370300293 - Accuracy: 0.1325\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take the test data and shift its contents to the right by p pixels\n",
    "p = 5\n",
    "x_test_roll = np.roll(x_test.reshape(-1, 28, 28), p, axis=-1)\n",
    "plt.subplot(121)\n",
    "plt.imshow(x_test[0].reshape(28, 28), cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.imshow(x_test_roll[0], cmap='gray');\n",
    "\n",
    "# Evaluate the model on the shifted data\n",
    "x_test_roll = x_test_roll.reshape(-1, 28 * 28)\n",
    "scores = model.evaluate(x_test_roll, y_test)\n",
    "print('Test loss: {} - Accuracy: {}'.format(*scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lo0ou80gBDuS"
   },
   "source": [
    "## Convolutional neural networks\n",
    "\n",
    "To solve the issue of translation invariance, one possibility is to adopt **convolutional neural networks** (CNNs) instead of the classical dense ones. \n",
    "\n",
    "CNNs were first introduced by Kunihiko Fukushima in 1980, and were later popularized by Y. LeCun, when he successfully applied backpropagation to train CNNs on MNIST. \n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png)\n",
    "\n",
    "Let's re-build our classifier from scratch, using convolutional layers instead of fully connected ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UOD-ivnECrKZ"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6di-USScCxa6"
   },
   "source": [
    "We still normalize the data to the 0-1 range, but this time we do not reshape the images into vectors. \n",
    "Instead, we add a new dimension which explicitly represents the different channels of our images. In the case of MNIST, we only have one 8-bit channel, so we only need to add a \"fake\" dimension at the of our data in order to have a 4D tensor of shape `(None, 28, 28, 1)`. \n",
    "If we had RGB images, they would be represented as tensors of shape `(None, 28, 28, 3)`.\n",
    "\n",
    "We also one-hot encode the labels as we did before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0cYp1r-DbzP"
   },
   "outputs": [],
   "source": [
    "# Normalize to 0-1 range\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "# Add channels dimension\n",
    "x_train = x_train[..., None]\n",
    "x_test = x_test[..., None]\n",
    "\n",
    "# Pre-process targets\n",
    "n_classes = 10\n",
    "y_train = utils.to_categorical(y_train, n_classes)\n",
    "y_test = utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9fPRpDhDd0v"
   },
   "source": [
    "So far the only difference from the previous model is the way in which we take the input. Let's see how to build a model that knows how to process these images using convolutional layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fFjHVNQwDrhE"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, 3, activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(32, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy',  \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 24, 24, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 596,042\n",
      "Trainable params: 596,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FUi12s7ID0mU"
   },
   "source": [
    "To train and evaluate the model, we do exactly as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qeak03BfD86z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 6784/60000 [==>...........................] - ETA: 41s - loss: 0.4403 - acc: 0.8631"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs)\n",
    "\n",
    "# Evaluate model\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print('Test loss: {} - Accuracy: {}'.format(*scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rKwgS3cZJ85R"
   },
   "source": [
    "Now let's see if the CNN is actually more robust to translations w.r.t. the dense net. We can run the same test as before, by shifting images to the right and evaluating the performance on the shifted test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1YrDP_R0IlTz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 61us/step\n",
      "Test loss: 1.2425784919396043 - Accuracy: 0.8039\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take the test data and shift its contents to the right by p pixels\n",
    "p = 2\n",
    "x_test_roll = np.roll(x_test, p, axis=2)\n",
    "plt.subplot(121)\n",
    "plt.imshow(x_test[0, ..., 0], cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.imshow(x_test_roll[0, ..., 0], cmap='gray');\n",
    "\n",
    "# Evaluate the model on the shifted data\n",
    "scores = model.evaluate(x_test_roll, y_test)\n",
    "print('Test loss: {} - Accuracy: {}'.format(*scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check some of the misclassified images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 2))\n",
    "cols = 4\n",
    "miss = 0\n",
    "for i in range(1000):\n",
    "    x = x_test[i]\n",
    "    y = np.argmax(y_test[i])\n",
    "    y_hat = np.argmax(model.predict(x[None]).squeeze())\n",
    "    if y != y_hat:\n",
    "        ax = fig.add_subplot(1, 5, miss + 1)\n",
    "        plt.title('Real class: {} , Predicted class: {}'.format(str(y), str(y_hat)))\n",
    "        plt.imshow(x.squeeze(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        miss += 1\n",
    "    if miss >= cols:\n",
    "        plt.show()\n",
    "        miss = 0\n",
    "        fig = plt.figure(figsize=(20, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UDzxs62EJRoU"
   },
   "source": [
    "## Extra : Fine-tuning VGG16\n",
    "\n",
    "![alt text](https://neurohive.io/wp-content/uploads/2018/11/vgg16.png)\n",
    "\n",
    "VGG16 is a 16-layer version of the network used by the VGG team in the ILSVRC-2014 (ImageNet) competition. \n",
    "It was one of the first networks that could be properly defined \"deep\", and it is still used today as a pre-packaged deep architecture for computer vision problems. \n",
    "\n",
    "Due to its large number of layers, VGG16 is extremely expensive to train, and requires several hours of compute time on millions of image samples in order to reach a decent classification performance. \n",
    "\n",
    "However, after the 2014 competition, the VGG team publicly released the **trained weights** of the network, so that people could use it for classifiying images without needing to re-train the whole net.   \n",
    "This also means that, while the whole network can only be used for classifying the original 1000 classes of the ILSVRC competition, the convolutional layers of the network can be used as **general-purpose feature extractors** to represent any natural image as a vector. \n",
    "\n",
    "By taking the output of the last pooling layer (or any other intermediate layer), we get an abstract description of the input images, which we then can use as input to a classical dense classifier designed specifically for our task. This process is called **fine-tuning**.\n",
    "\n",
    "For instance, in the following exercise we will take the convolutional block of VGG16, and we will feed its output to a simple binary classifier. This will allow us to train a model for recognizing cats and dogs, using the pre-trained VGG16 as feature extractor for a simpler network that is significantly less expensive to train than the whole VGG16. \n",
    "\n",
    "----\n",
    "\n",
    "For this exercise, we will see some more sophisticated Keras utils for automating the process of loading images from disk. We will also see how we can **augment** our dataset by transforming our images to obtain more variety in the data (e.g., we can flip all images horizontally).\n",
    "\n",
    "Let's start, as always, from the data. \n",
    "We've uploaded a reduced version of [this dataset](https://www.kaggle.com/c/dogs-vs-cats/data) to a SwitchDrive account, and we can download it directly into Colab by using `wget`.\n",
    "\n",
    "The data will be divided in folders as follows: \n",
    "\n",
    "```\n",
    "data/\n",
    " |__train/\n",
    "     |__cats/  # 1000 images\n",
    "     |__dogs/  # 1000 images\n",
    " |__val/\n",
    "     |__cats/  # 500 images\n",
    "     |__dogs/  # 500 images\n",
    " |__test/\n",
    "     |__cats/  # 500 images\n",
    "     |__dogs/  # 500 images\n",
    "```\n",
    "Note that labels are inherently provided by the folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b5fznCUPPZ4e"
   },
   "outputs": [],
   "source": [
    "# Download the data\n",
    "!wget https://drive.switch.ch/index.php/s/1YR2C31sSkac3gk/download -O data.zip\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6SckCPuPikd"
   },
   "source": [
    "We can use Keras' `ImageDataGenerator` class to create an object that will allow us to do several things:\n",
    "\n",
    "1. Read the images from the folders in batches, without keeping the whole dataset in memory;\n",
    "2. Automatically resize, rotate, shift, shear, zoom, and flip the images for us in order to perform data augmentation;\n",
    "3. Automatically create label vectors from the folder structure.\n",
    "\n",
    "See [here](https://keras.io/preprocessing/image/#imagedatagenerator-class) for all possible options for image augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tbxZPPMwQEDa"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import math\n",
    "\n",
    "img_h, img_w = 150, 150  # Dimensions to which we want to rescale our images\n",
    "batch_size = 16          # We specify the batch size here, so that the generator will know how many files to read\n",
    "\n",
    "# Create the ImageDataGenerator for training and val/test images.\n",
    "# Note that we only need to augment the training data.\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# Create generators for the three data splits\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'data/train',\n",
    "    target_size=(img_h, img_w),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    'data/val',\n",
    "    target_size=(img_h, img_w),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'data/test',\n",
    "    target_size=(img_h, img_w),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igoHF6l1R1_a"
   },
   "source": [
    "The VGG16 network architecture and weights [can be easily found online](https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3), but because it's so popular, it is also included by default in Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yBwLZCgOSMEH"
   },
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "\n",
    "# Build the VGG16 network and download pre-trained weights.\n",
    "# We remove the last dense layers because we will train our own. The rest of the\n",
    "# network will be \"frozen\", i.e., made not trainable.\n",
    "vgg16 = applications.VGG16(weights='imagenet',  \n",
    "                           include_top=False, \n",
    "                           input_shape=(img_h, img_w, 3))\n",
    "vgg16.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jz9U6J-HSReU"
   },
   "source": [
    "We can now use the convolutional part of VGG16 as if it were any other `keras.Layer` (in fact, `keras.Model` inherits from `keras.Layer`). Note that in the last line of code of the previous block, we set the loaded VGG16 model as not trainable. This means that the weights of VGG16 will not be updated during backpropagation (the network is still differentiable, but we do not take the gradient w.r.t. those weights).\n",
    "\n",
    "Let's build a classifier based on the pre-trained VGG16: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rEuGzid_Srd9"
   },
   "outputs": [],
   "source": [
    "# Build a classifier model to put on top of VGG16 convolutional layers\n",
    "model = Sequential()\n",
    "model.add(vgg16)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WgOSEFjxTQg5"
   },
   "source": [
    "And now we train it as usual, except that this time we use the `fit_generator` method of `Model`, and the `generator` object created by `ImageDataGenerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tk1_gsI0TUGK"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "n_train_samples = 2000\n",
    "n_val_samples = 1000\n",
    "n_test_samples = 1000\n",
    "epochs = 10\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=math.ceil(n_train_samples / batch_size),\n",
    "                    epochs=epochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=math.ceil(n_val_samples / batch_size))\n",
    "\n",
    "# Evaluate model\n",
    "scores = model.evaluate_generator(test_generator, steps=math.ceil(n_test_samples / batch_size))\n",
    "print('Test loss: {} - Accuracy: {}'.format(*scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GB_OeBU4VMEe"
   },
   "source": [
    "**Fun fact**: when the \"Cats vs. Dogs\" challenge was first released on Kaggle in 2014, computer vision experts posited that a classifier with better than 80% accuracy would be difficult to achieve without a major advance in the state of the art [[source]](https://www.kaggle.com/c/dogs-vs-cats/overview).\n",
    "\n",
    "As a final step, we can also use the model to classify a single test image as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEatX63cXRJh"
   },
   "outputs": [],
   "source": [
    "# Load a cat image\n",
    "from skimage import io, transform\n",
    "img = io.imread('data/test/cats/cat.1999.jpg')\n",
    "img = transform.resize(img, (img_h, img_w))\n",
    "plt.imshow(img);\n",
    "img = img[None, ...]\n",
    "\n",
    "pred = model.predict(img)[0, 0]\n",
    "pred_class = int(np.round(pred))\n",
    "pred_proba = (1 - pred_class) * (1 - pred) + pred_class * pred\n",
    "print('Image was predicted as being of class {} with probability {:2.2f}%.'.format(pred_class, pred_proba * 100))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "03_deep_learning",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
