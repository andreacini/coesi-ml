{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/andreacini/coesi-ml/blob/master/02_nonlinear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dtqk_bGrBFWt"
   },
   "source": [
    "# Excercise 02: Neural networks in Python\n",
    "\n",
    "To build our neural network we will use [TensorFlow](https://www.tensorflow.org/), one of the most popular deep learning libraries for Python (the other being [PyTorch](https://pytorch.org/)). \n",
    "TensorFlow provides a huge number of functions, like Numpy, that can be used to manipulate arrays, but offers two great advantages w.r.t. Numpy: \n",
    "\n",
    "1. the computation can be accelerated on GPU via the CUDA library;\n",
    "2. the library implements __automatic differentiation__, meaning that the most analytically complex step of training, the computation of the gradient, is handled for you.\n",
    "\n",
    "While TensorFlow is a very powerful library that offers a fine-coarsened control over what you build, for this course we will skip the low level details and instead use the official high-level API for TensorFlow: [Keras](https://keras.io).\n",
    "\n",
    "# Introduction to TensorFlow/Keras\n",
    "\n",
    "![alt text](https://www.tensorflow.org/images/tf_logo_social.png)\n",
    "\n",
    "(most of what follows is adapted from [the introduction on the TensorFlow website](https://www.tensorflow.org/guide/low_level_intro))\n",
    "\n",
    "The most important things to understand about building neural networks with Tensorflow, are the concepts of __tensor__ and __computational graph__. \n",
    "\n",
    "## Tensors\n",
    "A tensor consists of a set of primitive values shaped into an array of any number of dimensions. A tensor's __rank__ is its number of dimensions, while its __shape__ is a tuple of integers specifying the array's length along each dimension. \n",
    "\n",
    "Intuitively, tensors are the TensorFlow version of Numpy arrays. In fact, TF and Numpy are heavily intertwined and arrays can often be fed to TF models without modifications. \n",
    "\n",
    "## Computational graph\n",
    "A computational graph is a series of TensorFlow operations arranged into a graph. The graph is composed of two types of objects.\n",
    "\n",
    "- `tf.Operation` (or \"ops\"): The nodes of the graph. Operations describe calculations that consume and produce tensors.\n",
    "- `tf.Tensor`: The edges in the graph. These represent the values that will flow through the graph. Most TensorFlow functions return tf.Tensors.\n",
    "\n",
    "\n",
    "For this exercise session (and most of the course, probably), you will only need to keep in mind one thing: when using TF, __you first define the computation, and then you provide the data__. \n",
    "\n",
    "This means that all of your operations will be defined on symbolic objects, and only at the end you will actually run the computation.\n",
    "\n",
    "Don't worry if you don't get this at first, it will become clearer by doing. \n",
    "\n",
    "# Keras\n",
    "\n",
    "![alt text](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)\n",
    "\n",
    "\n",
    "\n",
    "Keras offers collections of TF operations already arranged to implement neural networks with little to no effort. \n",
    "For instance, building a layer of 4 neurons like the one we saw above is as easy as calling `Dense(4)`. That's it. \n",
    "\n",
    "Moreover, Keras offers a high-level API for doing all the usual steps that we usually do when training a neural network, like training on some data, evaluating the performance, and predicting on unseen data. \n",
    "\n",
    "The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the `Sequential` model, a linear stack of layers.\n",
    "\n",
    "---\n",
    "\n",
    "In this exercise we will see how to build and train a simple neural network from scratch, in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jQ3Da3hk7bJ9"
   },
   "source": [
    "# Boston dataset\n",
    "\n",
    "Let's build a model to predict the prices of houses in the Boston area in the 70ties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Set Characteristics:**  \n",
    "\n",
    "1. mpg: continuous\n",
    "2. cylinders: multi-valued discrete\n",
    "3. displacement: continuous\n",
    "4. horsepower: continuous\n",
    "5. weight: continuous\n",
    "6. acceleration: continuous\n",
    "7. model year: multi-valued discrete\n",
    "8. origin: multi-valued discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "import keras\n",
    "\n",
    "dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
    "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "raw_dataset = pd.read_csv(dataset_path, names=column_names,\n",
    "                      na_values = \"?\", comment='\\t',\n",
    "                      sep=\" \", skipinitialspace=True)\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "dataset.dropna(inplace=True)\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Origin'] = dataset['Origin'].map(lambda x: {1: 'USA', 2: 'Europe', 3: 'Japan'}.get(x))\n",
    "dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = dataset.corr()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eVkInghm-LIV"
   },
   "source": [
    "Notice how the values of the features are not commensurable with one another.\n",
    "\n",
    "While this in principle is not a problem for our machine learning models, in practice it can lead to issues in the training procedure.\n",
    "\n",
    "To standardize the data, we compute the following transformation: \n",
    "\n",
    "$$\n",
    "X_{\\textrm{standardized}} = \\frac{X - \\textrm{mean}(X)}{\\textrm{std}(X)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrUH22Z87x6k"
   },
   "outputs": [],
   "source": [
    "# Extract features\n",
    "X = dataset[dataset.columns[1:]].values\n",
    "\n",
    "# Normalize features\n",
    "X -= np.mean(X, axis=0)\n",
    "X /= np.std(X, axis=0)\n",
    "\n",
    "# Extact targets\n",
    "y = dataset['MPG'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rCfbPIHWzWEG"
   },
   "source": [
    "In order to train our network, we will spliit the data into three main sets:\n",
    "\n",
    "- training, which we will use to train the mode\n",
    "- validation, which we use to monitor the performance of the model while training\n",
    "- test, which we use to evaluate the model at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tl3yxnz3zsbl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(20)\n",
    "# Split train / test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OkjyJB0Jz5Yt"
   },
   "source": [
    "Now that we have loaded and pre-processed our data, we only need to build the neural network that we will train. Using keras, this is achieved in a few lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6jtB0WD-9W9S"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Define the network\n",
    "network = Sequential()\n",
    "network.add(Dense(32, activation='tanh', input_shape=X.shape[1:]))\n",
    "network.add(Dense(1))\n",
    "\n",
    "# Choose an optimizer\n",
    "\n",
    "optim = keras.optimizers.SGD(lr=0.01)\n",
    "\n",
    "# Prepare the computational graph and training operations\n",
    "network.compile(optimizer='sgd', \n",
    "                loss='mse',\n",
    "                metrics=['mae'])\n",
    "\n",
    "# Train the network\n",
    "network.fit(X_train, y_train, batch_size=64, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance\n",
    "eval_results = network.evaluate(X_test, y_test)\n",
    "print('Test mae: {}'.format(eval_results[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the error distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = network.predict(X_test)\n",
    "err = y_test.values - y_hat.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "# plot error distribution\n",
    "sns.distplot(err, hist=True, norm_hist=False, label='error', bins=20)\n",
    "plt.xlim(-11,11)\n",
    "plt.vlines(0, 0., 0.2, linestyles='--')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "02_classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
