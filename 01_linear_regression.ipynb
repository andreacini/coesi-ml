{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MesOM1nHi52O"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/danielegrattarola/ml-18-19/blob/master/01_linear_regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbklJz5msK5z"
   },
   "source": [
    "# Machine Learning\n",
    "\n",
    "Prof. Cesare Alippi  \n",
    "Daniele Grattarola  ([`daniele.grattarola@usi.ch`](mailto:daniele.grattarola@usi.ch)  )    \n",
    "Daniele Zambon ([`daniele.zambon@usi.ch`](mailto:daniele.zambon@usi.ch)  )\n",
    "\n",
    "---\n",
    "# Lab 01: Linear Regression\n",
    "\n",
    "System model\n",
    "$$y = \\beta_1  x_1 + \\beta_2  x_2 + \\dots + \\beta_d  x_d + \\varepsilon = \\mathbf x^\\top \\beta +\\varepsilon$$\n",
    "\n",
    "Data set of $n$ observations\n",
    "$$\\{(\\mathbf x_1, y_1), (\\mathbf x_2, y_2) ,\\dots,(\\mathbf x_n, y_n)\\}.$$\n",
    "\n",
    "Linear regression model \n",
    "$$\n",
    "\\begin{cases}\n",
    "y = \\mathbf x^\\top \\beta +\\varepsilon\\\\\n",
    "\\varepsilon \\sim N(0, \\sigma^2)\\quad i.i.d.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Q)** Given an observation $\\mathbf x$, what is our best prediction for $y$?\n",
    "\n",
    "**A)** If we know $\\beta$ then \n",
    "$$\\hat y = E[y] = \\mathbf x^\\top \\beta + E[\\varepsilon] = \\mathbf x^\\top \\beta.$$\n",
    "\n",
    "Here is how we proceed:\n",
    "1. We estimate $\\beta$ on a training set by minimising \n",
    "$$\\hat \\beta = {\\rm arg}{\\min}_{\\beta} \\sum_{i=1}^n \\lVert y_i - \\mathbf x_i^\\top \\beta \\rVert^2_2$$\n",
    "2. We predict new responses with \n",
    "$$\\hat y = \\mathbf x^\\top \\hat \\beta.$$\n",
    "\n",
    "\n",
    "## More generally\n",
    "\n",
    "* The response $y$ can be a vector $\\mathbf y\\in\\mathbb{R}^m$;\n",
    "* The regressor can be of the form \n",
    "$$\\mathbf x' = [f_1(\\mathbf x), f_2(\\mathbf x), \\dots, f_d(\\mathbf x)]^\\top$$ \n",
    "\n",
    "We result in \n",
    "$$\\mathbf y = \\beta_1 f_1(\\mathbf x) + \\beta_2 f_d(\\mathbf x) + \\dots + \\beta_d f_d(\\mathbf x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qv4BK5Y_SY4p"
   },
   "source": [
    "## 01.A: Ordinary Linear Regression\n",
    "\n",
    "Consider the univariate case $f : \\mathbb{R} \\rightarrow \\mathbb{R}$, we all know that functions of the form\n",
    "$$ f(x; \\beta) = \\beta_1 x + \\beta_0, \\quad \\beta_1, \\beta_0\\in\\mathbb{R}$$\n",
    "are all lines in the plane.\n",
    "\n",
    "Define in python the function\n",
    "$$ f(x) = \\tfrac{3}{10} x + 2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OleyzwMnPuMI"
   },
   "outputs": [],
   "source": [
    "def lin_fun(x):\n",
    "    y = 0.3 * x + 2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "711op_bXAjLg"
   },
   "source": [
    "\n",
    "\n",
    "Plot the graph of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NU23MlsCSAcd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create n observations\n",
    "n = 11 \n",
    "x = np.linspace(-1, 1, n)\n",
    "print('x = {}'.format(x))\n",
    "\n",
    "# plot\n",
    "plt.plot(x, lin_fun(x), '*-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V1S-jSYeUJ0w"
   },
   "source": [
    "### Noise perturbance\n",
    "\n",
    "Consider a model affected by Gaussian noise: $$ y = f(x) + \\varepsilon, \\quad\\varepsilon\\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "Generate $n$ observations: $$\\{(x_1,y_1),(x_2,y_2),\\dots,(x_n,y_n)\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vV1OYLzZT2eT"
   },
   "outputs": [],
   "source": [
    "# seed for the pseudo-random generator\n",
    "np.random.seed(1233)\n",
    "\n",
    "n = 20  # number of data points\n",
    "\n",
    "# regressor\n",
    "x = np.linspace(-1, 1, n)  \n",
    "# noise\n",
    "sigma = 0.1  # std of the noise\n",
    "eps = np.random.normal(loc=0, scale=sigma, size=n)\n",
    "# response\n",
    "y = lin_fun(x) + eps\n",
    "\n",
    "# plot\n",
    "plt.plot(x, lin_fun(x), label='true fun');  # original linear function\n",
    "plt.scatter(x, y, label='noisy data');      # data affected by noise\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PfdvHYsQiKSb"
   },
   "source": [
    "## Parameter estimation\n",
    "\n",
    "Assume to have the dataset $\\{(x_1,y_1),(x_2,y_2),\\dots,(x_n,y_n)\\}$, but not to know the system model.\n",
    "Let's try to recover the parameters from the noisy data by solving:\n",
    "$$\n",
    "\\hat\\beta={\\rm arg}\\min_{\\beta} \\sum_{i=1}^n \\lVert f(x_i;\\beta) -y_i\\rVert^2.\n",
    "$$\n",
    "\n",
    "### Data in compact form\n",
    "We can express the function $f(x;\\beta)$ in the form\n",
    "$$\n",
    "f(x;\\beta) = \\beta_1 x + \\beta_0 = \\left[f_1(x), f_0(1) \\right] \\left[\\begin{array}{c}\\beta_1 \\\\ \\beta_0 \\end{array} \\right]  = \\left[x, 1 \\right] \\left[\\begin{array}{c}\\beta_1 \\\\ \\beta_0 \\end{array} \\right] \n",
    "$$\n",
    "with $f_1(x) = x$ and $f_0(x)=1$.\n",
    "\n",
    "We showed in class that the solution can be found by solving the linear system\n",
    "$$\n",
    "X^\\top Y = X^\\top X \\beta \n",
    "$$\n",
    "with respect to $\\beta$, where we arranged the data in matrix form as follows\n",
    "$$\n",
    "X = \\left[ \n",
    "\\begin{array}{c}\n",
    "x_1, 1 \\\\\n",
    "x_2 , 1 \\\\\n",
    "\\vdots \\\\\n",
    "x_n, 1\n",
    "\\end{array}\n",
    "\\right] \n",
    "\\in \\mathbb{R}^{n\\times 2},\n",
    "\\qquad \n",
    "Y = \\left[ \n",
    "\\begin{array}{c}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{array}\n",
    "\\right] \n",
    "\\in \\mathbb{R}^{n}\n",
    "$$\n",
    "\n",
    "### Estimated parameters $\\beta$\n",
    "* When $X^\\top X$ is invertible, the solution is\n",
    "$$\\hat\\beta = (X^\\top X)^{-1}X^\\top Y = X^+ Y$$\n",
    "**Notice**: _matrix $X^+=(X^\\top X)^{-1} X^\\top$ is called_ [pseudo-inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) _of $X$_.\n",
    "* Otherwise, we solve with respect to $\\beta$ the system\n",
    "$$X^\\top X\\, \\beta = X^\\top Y.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gG0mOT_NizuW"
   },
   "outputs": [],
   "source": [
    "# construct matrix X\n",
    "ones_vec = np.ones((n, 1))  \n",
    "x = x.reshape(n, 1)\n",
    "X = np.concatenate((x, ones_vec), axis=1)\n",
    "print('X = ')\n",
    "print(X[:5])  # only the first 5 data points\n",
    "\n",
    "# parameter estimation\n",
    "S = X.T @ X\n",
    "Sinv = np.linalg.inv(S)\n",
    "beta_est = Sinv @ X.T @ y\n",
    "y_est = X @ beta_est\n",
    "\n",
    "# plot\n",
    "plt.plot(x, lin_fun(x), label='true fun');  # original linear function\n",
    "plt.scatter(x, y, label='noisy data');      # data affected by noise\n",
    "plt.plot(x, y_est, label='est fun');  \n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KjQuvkZwmWhl"
   },
   "source": [
    "### Scikit-Learn: a library for machine learning in Python\n",
    "\n",
    "The same procedure can be done with `sklearn` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TjTYT48Rmfj1"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# init the model\n",
    "lr = LinearRegression()  \n",
    "# estimate parameters\n",
    "lr.fit(x, y)\n",
    "beta_est = [lr.coef_[0], lr.intercept_]\n",
    "print('beta = {}'.format(beta_est))\n",
    "# estimated response\n",
    "y_est = lr.predict(x)\n",
    "\n",
    "# plot\n",
    "plt.plot(x, lin_fun(x), label='true fun');      # original linear function\n",
    "plt.scatter(x, y, label='noisy data');          # data affected by noise\n",
    "plt.plot(x, y_est, label='est fun (sklearn)');  # estimate linear function\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iHvZnV1rWmwq"
   },
   "source": [
    "<!-- # Multivariate case\n",
    "Define a $d$-dimensional linear function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ -->\n",
    "\n",
    "## Bi-dimensional case $f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}$\n",
    "$$f(\\mathbf x; \\beta) = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_0$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sAIiU7FdV9NF"
   },
   "outputs": [],
   "source": [
    "def lin_fun_2(x):\n",
    "    y = 0.1 * x[:, 0] - 1.3 * x[:, 1] + 1\n",
    "    return y\n",
    "\n",
    "def lin_fun_2(x):\n",
    "    beta = np.array([0.1, -1.3])  # coefficients\n",
    "    beta0 = 1                     # intercept\n",
    "    y = x @ beta + beta0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KwrhuVLndrBz"
   },
   "source": [
    "## Generic $d$-dimensional case $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$\n",
    "$$f(\\mathbf x; \\beta) = \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_d x_d$$\n",
    "\n",
    "\n",
    "### Data in matrix form\n",
    "We generate $n$ observations: $\\{(\\mathbf x_1,y_1),(\\mathbf x_2,y_2),\\dots,(\\mathbf x_n,y_n)\\}$\n",
    "and store them in matrix form:\n",
    "$$\n",
    "X = \\left[ \n",
    "\\begin{array}{c}\n",
    "\\mathbf x_1^\\top \\\\\n",
    "\\mathbf x_2^\\top \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf x_n^\\top\n",
    "\\end{array}\n",
    "\\right] \n",
    "\\in \\mathbb{R}^{n\\times d},\n",
    "\\qquad \n",
    "Y = \\left[ \n",
    "\\begin{array}{c}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{array}\n",
    "\\right] \n",
    "\\in \\mathbb{R}^{n}\n",
    "$$\n",
    "\n",
    "**Remark 1:** _`sklearn` deals with the intercept for us, so we do not need to incorporate the '1' in $\\mathbf x$. There is a flag, if you don't want it, see the_\n",
    "[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear\\_model.LinearRegression.html)  \n",
    "\n",
    "```python\n",
    "class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
    "```\n",
    "\n",
    "**Remark 2:**\n",
    "_Often data is normalised before being processed. Modelled with functions $\\{f_i(\\cdot)\\}$:_ \n",
    "$$f(x; \\beta) = \\beta_1 f_1(x_1) + \\beta_2 f_2(x_2) + \\dots + \\beta_d f_d(x_d).$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCgY08NMbbFH"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# a random d-dimensional example\n",
    "d = 2  \n",
    "beta = np.random.uniform(-1, 1, d)\n",
    "beta0 = np.random.uniform(-1, 1)\n",
    "def lin_fun_d(X):\n",
    "    return X @ beta + beta0\n",
    "\n",
    "# generate data\n",
    "n = 100\n",
    "X = np.random.uniform(-1, 3, size=(n, 2)) \n",
    "sigma = 0.1\n",
    "eps = np.random.normal(loc=0, scale=sigma, size=n)\n",
    "Y = lin_fun_d(X) + eps\n",
    "print('size of X = {}'.format(X.shape))\n",
    "print('size of Y = {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NM9wF2NTimAN"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  # this is necessary for 3-d plots \n",
    "\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d');\n",
    "ax.set_xlabel('x_0')\n",
    "ax.set_ylabel('x_1')\n",
    "ax.set_zlabel('y')\n",
    "ax.scatter(X[:, 0], X[:, 1], Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "662Hsnw2ocHZ"
   },
   "outputs": [],
   "source": [
    "# estimate the parameters\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, Y)\n",
    "Y_est = lr.predict(X)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d');\n",
    "ax.scatter(X[:, 0], X[:, 1], Y, label='noisy data');\n",
    "ax.plot_trisurf(X[:, 0], X[:, 1], Y_est, alpha=0.3, label='est fun');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lpzPRkKuhWv5"
   },
   "source": [
    "\n",
    "## Dealing with multi-dimensional outputs $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}^m$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "y_1&= \\beta_{11} x_1 + \\beta_{21} x_2 + \\dots + \\beta_{d1} x_d + \\varepsilon_1\n",
    "\\\\\n",
    "y_2&= \\beta_{12} x_1 + \\beta_{22} x_2 + \\dots + \\beta_{d2} x_d + \\varepsilon_2\n",
    "\\\\\n",
    "&\\vdots\n",
    "\\\\\n",
    "y_m&= \\beta_{1m} x_1 + \\beta_{2m} x_2 + \\dots + \\beta_{dm} x_d + \\varepsilon_m\n",
    "\\end{cases}\n",
    "$$\n",
    "in matrix form \n",
    "$$ \\mathbf y = B\\, \\mathbf x + \\mathbf \\varepsilon$$\n",
    "<!-- or equivalently\n",
    "$$ \\mathbf y^\\top = \\mathbf x^\\top B + \\mathbf \\beta_0^\\top$$ -->\n",
    "where\n",
    "$$\n",
    "\\mathbf y = \\left[ \\begin{array}{c}\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "\\vdots \n",
    "\\\\\n",
    "y_{m}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\qquad\n",
    "\\mathbf x = \\left[ \\begin{array}{c}\n",
    "x_{1}\\\\\n",
    "x_{2}\\\\\n",
    "\\vdots \n",
    "\\\\\n",
    "x_{d}\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "B = \\left[ \\begin{array}{c}\n",
    "\\beta_{11}, &\\beta_{21}, &\\dots, &\\beta_{d1}\\\\\n",
    "\\beta_{12}, &\\beta_{22}, &\\dots, &\\beta_{d2}\\\\\n",
    "\\vdots  & \\vdots &\\ddots& \\vdots\n",
    "\\\\\n",
    "\\beta_{1m}, &\\beta_{2m}, &\\dots, &\\beta_{dm}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\qquad\n",
    "\\varepsilon = \\left[ \\begin{array}{c}\n",
    "\\varepsilon_{1}\\\\\n",
    "\\varepsilon_{2}\\\\\n",
    "\\vdots \n",
    "\\\\\n",
    "\\varepsilon_{m}\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsMebHABqi7N"
   },
   "source": [
    "## 01.B: Approximate polynomials with linear regression\n",
    "\n",
    "$$f(x; \\beta) =\\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_d x^d $$\n",
    "\n",
    "<font color='gray'>__Remark:__ _This is linear in the parameters! In fact, 'linear' in 'linear regression' refers to the parameters. \n",
    "As we said, a more general case is_\n",
    "$$f(x; \\beta) = \\beta_1 f_1(\\mathbf x) + \\beta_2 f_2(\\mathbf x) + \\dots + \\beta_d f_d(\\mathbf x)$$\n",
    "</font>\n",
    "\n",
    "Get back to the polynomials. We consider a regressor vector $[x, x^2,\\dots,  x^d]$, up to a degree $d$ that we can decide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tecbvi2mqh0m"
   },
   "outputs": [],
   "source": [
    "def pol_fun(x):\n",
    "    return -1 + -x - .1 * x**2 + .5*x**3\n",
    "\n",
    "# generate data\n",
    "n = 200\n",
    "X = np.linspace(-1, 2, n).reshape(n,1) \n",
    "sigma = 0.3\n",
    "eps = np.random.normal(loc=0, scale=sigma, size=(n,1))\n",
    "Y = pol_fun(X) + eps\n",
    "\n",
    "# create regressor\n",
    "degree = 10\n",
    "# Xpol = np.concatenate((X, X**2, X**3), axis=1)\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "pol_feat = PolynomialFeatures(degree=degree, include_bias=False) \n",
    "Xpol = pol_feat.fit_transform(X)\n",
    "\n",
    "# estimate parameter\n",
    "lr = LinearRegression()\n",
    "lr.fit(Xpol, Y)\n",
    "Y_est = lr.predict(Xpol)\n",
    "\n",
    "# plot results\n",
    "plt.plot(X, pol_fun(X), label='true fun');\n",
    "plt.scatter(X, Y, label='noisy data');\n",
    "plt.plot(X, Y_est, label='est fun');\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9UCZmGCj6WG"
   },
   "source": [
    "## 01.C: Ridge regression\n",
    "\n",
    "$$\n",
    "\\hat\\beta={\\rm arg}\\min_{\\beta} \\sum_{i=1}^n \\lVert f(x_i;\\beta) -y_i\\rVert^2_2 + \\lambda \\lVert \\beta\\rVert^2_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxSmRiArvnlJ"
   },
   "outputs": [],
   "source": [
    "# generate data [200 100 50 20 10]\n",
    "n = 200\n",
    "X = np.linspace(-1, 2, n).reshape(n,1) \n",
    "sigma = 0.3\n",
    "eps = np.random.normal(loc=0, scale=sigma, size=(n,1))\n",
    "Y = pol_fun(X) + eps\n",
    "\n",
    "# create regressor\n",
    "degree = 5\n",
    "pol_feat = PolynomialFeatures(degree=degree, include_bias=False) \n",
    "Xpol = pol_feat.fit_transform(X)\n",
    "\n",
    "# linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(Xpol, Y)\n",
    "Y_est = lr.predict(Xpol)\n",
    "\n",
    "# ridge regression\n",
    "from sklearn.linear_model import Ridge \n",
    "rid = Ridge()\n",
    "rid.fit(Xpol, Y)\n",
    "Y_rid_est = rid.predict(Xpol)\n",
    "\n",
    "# plot results\n",
    "plt.plot(X, pol_fun(X), label='true fun')\n",
    "plt.scatter(X, Y, label='noisy data')\n",
    "plt.plot(X, Y_est, label='lin.reg.')\n",
    "plt.plot(X, Y_rid_est, label='ridge reg.')\n",
    "plt.legend()\n",
    "\n",
    "# estimated beta\n",
    "print('beta_lr  = {}'.format(lr.intercept_ + lr.coef_[0]))\n",
    "print('beta_rid = {}'.format(rid.intercept_ + rid.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gTAe51Ai54w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dm0ud2tri542"
   },
   "source": [
    "## 01.D: Exercise: Boston house price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pF0_nh7Ji544"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(boston.DESCR)\n",
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3piFp1iVi55H"
   },
   "outputs": [],
   "source": [
    "# Try yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4kUZt7gi55S"
   },
   "source": [
    "## 01.E: Exercise: Lasso regression \n",
    "* __Use the documentation__: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "* Test on Boston house-price dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vycIYi8_i55U"
   },
   "outputs": [],
   "source": [
    "# Try yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCNolDOvi55b"
   },
   "source": [
    "## 01.F: More on linear regression\n",
    "### Confidence intervals for the parameters\n",
    "\n",
    "Assume that $X^\\top X$ is invertible, then\n",
    "\n",
    "$$\n",
    "\\hat \\beta = X^+Y \\sim N\\big(\\beta, \\sigma^2 (X^\\top X)^{-1}\\big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[\\hat \\beta] = E[X^+Y] = X^+E[Y] = X^+ X\\beta = (X^\\top X)^{-1}X^\\top X \\beta = \\beta \n",
    "$$\n",
    "\n",
    "$$\n",
    "Var[\\hat \\beta] = Var[X^+Y] = X^+Var[Y] (X^+)^\\top = \\sigma^2 (X^\\top X)^{-1} X^\\top X (X^\\top X)^{-1} = \\sigma^2 (X^\\top X)^{-1} \n",
    "$$\n",
    "\n",
    "### A rule of thumb\n",
    "\n",
    "* Extract the diagonal from $\\sigma^2 (X^\\top X)^{-1}$, which gives you an idea of the variance of each component of $\\beta$.\n",
    "* For each component $\\beta_i$, check if the interval $(\\beta_i - 2\\sigma_i, \\beta_i + 2\\sigma_i)$ contains the zero; is such case, we are not very confident that the $\\beta_i\\ne 0$, thus that $x_i$ is relevant in the model.\n",
    "\n",
    "\n",
    "### More formally\n",
    "\n",
    "Knowing that the distribution of $\\hat \\beta$ is a (multivariate) Gaussian distribution, we can obtain a confidence interval:\n",
    "$$\n",
    "CI = \\{\\hat \\beta\\ s.t.\\ d(\\hat\\beta, \\beta)^2 < \\chi^2_p(1-\\alpha)\\}\n",
    "$$\n",
    "such that $P(\\hat \\beta \\in CI) = 1-\\alpha$, and where\n",
    "$$\n",
    "d(\\hat\\beta, \\beta)^2 = (\\hat\\beta - \\beta)^\\top \\sigma^{-2} X^\\top X (\\hat \\beta - \\beta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "519gIfcAi55c"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_linear_regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
